通用代码优化见 https://github.com/pingz1988/Linux/blob/master/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/%E4%BC%98%E5%8C%96.md  
以下是关于DPDK应用程序优化  

## DPDK应用程序设计准则 
* 数据本地化  
* 无锁化

## 两种模式  
* pipline  
* run-to-completion  
在不同的应用需求下，一种模式的性能表现会比另一种强  

## 优化方法
1. 找出程序的热点代码（利用perf/VTune等工具）
2. 参考DPDK相关代码（包括配置、选项）进行优化，也可考虑改造DPDK提供的单元测试代码来对自己的应用程序测试。  
    2.1 用系统API去时间评估性能，系统开销太大。rte_rdtsc() 是个很好的实现，一条汇编指令，在怀疑性能的地方使用宏PERF_START、PERF_COUNT、PERF_DUMP
3. 重复上述步骤直到性能达到预期

# 非代码方面  
* BIOS设置  
* 操作系统feature开关  
* 分配大页
* CPU配置，收包/包处理分配的CPU是否跨NUMA
  
# 代码方面
## 内存
* 内存对齐   
  结构体按cache line大小64字节对齐，结构体中的成员变量按占用内存从大到小排列
* API  
  dpdk内部实现了内存相关的操作函数，考虑了内存对齐、NUMA、内存通道分布等等因素，它们更高效，尤其对于大内存分配的分配及操作，比如rte_malloc/rte_free/rte_memcpy/rte_strcpy，尽量使用这些函数，避免使用libc内存操作函数。类似地，还有rte_ring（lcore间的通信）/rte_mempool（大内存分配）/rte_hash 等
* 多lcore读写访问内存  
  不同lcore对同一块内存的进行读写访问会造成大量cache-miss，每个核单独使用的变量用 RTE_PER_LCORE 宏修饰，只是读访问的话不需要这个处理
* NUMA  
  lcore的使用需注意它所在的NUMA节点，跨NUMA访问，会造成cache-miss，rte_mempool/rte_ring/rte_malloc等都考虑了跨NUMA问题

## Cache
* 分支预测  
  使用likely/unlikey，多条件分支，按出现概率从大到小排列
* 数据预取  
  合理使用数据预取，**需谨慎使用，也不能过度使用，需反复测试，否则性能反而会下降**。  
  1、硬件预取，由硬件实现，略过  
  2、软件预取：处理批量数据时用rte_prefetch0()预取一个cache line数据到cache中，处理单个数据时用rte_prefetch_non_temporal()预取一个cache line数据到cache中。不要在内层循环的末尾使用（非末尾处可以使用）数据预取指令，在外层循环中预取内层循环数据。
  ```c
  for (ii = 0; ii < 100; ii++) {
    for (jj = 0; jj < 24; jj+=8) { /* N-1 iterations */
        prefetch a[ii][jj+8]
        computation a[ii][jj]
    }
    prefetch a[ii+1][0]
    computation a[ii][jj]/* Last iteration */
  }
  ```
  
## 无锁化  
   使用锁会带来性能损耗，可考虑使用单核变量（per-lcore）、RCU（Read-Copy-update，DPDK提供了RCU库，但文档警告这个库会在不事前通知的情况下被更改）算法替代读写锁（rwlock），从而实现无锁化  
   
## 线程/进程间通信  
  使用ring进行线程/进程间通信
  
## 数据结构
* mempool  
默认是基于ring方式，**基于stack方式的mempool效率较高？**

## 算法  

## 指令优化（待续）  
  有的CPU指令会优化特定的运算
